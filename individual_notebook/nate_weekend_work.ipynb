{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created nate_branch for phase 2 group project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: \n",
    "* Student pace: self paced / part time / full time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible stakeholder:\n",
    "Here's how much your home will sell for. Ask Shane about standard average  asking price -  haggle price = closing price so we can adjust our recommendation up that percent \n",
    "say asking_price = 100,000 and buyer haggles to 90,000, we would factor that in and recommend asking_price be 110,000\n",
    "\n",
    "object to datetime for this purpose. ['yr_renovated'] has 3842 Null values.\n",
    "\n",
    "our price mean is 540296.5735055795\n",
    "\n",
    "['price'] and ['zipcode'] have a very low correlation of -0.05340243437772487\n",
    "\n",
    "Top correlations with price\n",
    "sqft_living = 0.70\n",
    "grade = 0.66\n",
    "sqft_above = 0.60\n",
    "sqft_living15 = 0.58\n",
    "\n",
    "\n",
    "['yr_renovated'] has something in it that the second_model doesn't like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-617c257be7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfolium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeatMap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import gamma\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler, QuantileTransformer \n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, Ridge, RidgeCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "#validation libraries\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style('whitegrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into a variable called data\n",
    "data = pd.read_csv('data/kc_house_data.csv')\n",
    "\n",
    "# make a copy of data to work with calle df\n",
    "df = data.copy(deep=True)\n",
    "\n",
    "# converted dates from object to datetime\n",
    "df.date = pd.to_datetime(df.date,infer_datetime_format=True)\n",
    "df['date']=df['date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "# getting rid of a question mark in one of the columns\n",
    "df['sqft_basement'] = df['sqft_basement'].replace({'?':np.nan}).astype(float)\n",
    "df.head(50)\n",
    "\n",
    "# get rid of those pesky NaN\n",
    "df['yr_renovated'] = df['yr_renovated'].fillna(0)\n",
    "\n",
    "# convert to log_price \n",
    "df['log_price'] = np.log(df['price']) #$$$$$$\n",
    "\n",
    "# removing normalized price from our log_price df\n",
    "df.drop(['price'], axis=1, inplace=True) #$$$$$\n",
    "\n",
    "df['view'] = df['view'].fillna(0)\n",
    "\n",
    "# check log_price minimum\n",
    "df.log_price.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.waterfront = df.waterfront.fillna(0)\n",
    "\n",
    "df_dummies = pd.get_dummies(df, prefix='waterfront', prefix_sep='_',\n",
    "                           columns=['waterfront'], drop_first = True)\n",
    "dummy_waterfront = df_dummies['waterfront_1.0']\n",
    "\n",
    "\n",
    "df_dummies['dummy_waterfront'] = dummy_waterfront\n",
    "\n",
    "df_dummies.drop('waterfront_1.0', axis=1, inplace=True)\n",
    "\n",
    "df_dummies.head()\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Longitude and Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to deal with longitude and latitude\n",
    "# Round off after the decimal to help our model \n",
    "# df = df.round({\"long\":4, \"lat\":4})\n",
    "\n",
    "# **A second option to deal with long and lat**\n",
    "# Change long and lat to radians instead of degrees\n",
    "df[\"long\"] = np.radians(df[\"long\"])\n",
    "df[\"lat\"] = np.radians(df[\"lat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans color clustering the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means\n",
    "# https://towardsdatascience.com/visualizing-clusters-with-pythons-matplolib-35ae03d87489\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "df['cluster'] = kmeans.fit_predict(df[['lat', 'long']])\n",
    "# get centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "cen_x = [i[0] for i in centroids] \n",
    "cen_y = [i[1] for i in centroids]\n",
    "## add to df\n",
    "df['cen_x'] = df.cluster.map({0:cen_x[0], 1:cen_x[1], 2:cen_x[2]})\n",
    "df['cen_y'] = df.cluster.map({0:cen_y[0], 1:cen_y[1], 2:cen_y[2]})\n",
    "# define and map colors\n",
    "colors = ['#DF2020', '#81DF20', '#2095DF']\n",
    "df['c'] = df.cluster.map({0:colors[0], 1:colors[1], 2:colors[2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning our model variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_cols_for_work = [ 'id','sqft_lot', 'floors'\n",
    "                ,'waterfront', 'yr_built','sqft_basement', 'date'\n",
    "            ,'zipcode', 'yr_renovated', 'bathrooms', 'cen_x', 'cen_y']\n",
    "X_train.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = data.price\n",
    "y = df.log_price  #changed log_price to log Sunday night 8:40\n",
    "X = df.drop('log_price', axis=1) #changed log_price to log Sunday night 8:40\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2) # ,random_state=10) if you want the samples to stay the same each time\n",
    "\n",
    "# X_train.drop(['date'], axis=1, inplace=True)\n",
    "# X_test.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "# # changed ? in ['sqft_basement to NaN']\n",
    "# X_train['sqft_basement'] = X_train['sqft_basement'].replace({'?':np.nan}).astype(float)\n",
    "# X_test['sqft_basement'] = X_test['sqft_basement'].replace({'?':np.nan}).astype(float)\n",
    "X_train.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting out the locations of our homes over a map of King Co."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,8))\n",
    "sns.scatterplot(X_train.long, X_train.lat, alpha=.7);\n",
    "\n",
    "plt.title('Longitude and Latitude Coordinates of Houses in King County')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('latitude');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.scatter(df.long, df.lat, c=df.c, alpha = 0.6, s=10,)\n",
    "\n",
    "# create a list of legend elemntes\n",
    "## markers / records\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Cluster {}'.format(i+1), \n",
    "               markerfacecolor=mcolor, markersize=5) for i, mcolor in enumerate(colors)]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A heatmap to help inspect correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with the target as the first column,\n",
    "# then compute the correlation matrix\n",
    "heatmap_data = pd.concat([y_train, X_train], axis=1)\n",
    "corr = heatmap_data.corr()\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "# Plot a heatmap of the correlation matrix, with both\n",
    "# numbers and colors indicating the correlations\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=corr,\n",
    "    # The mask means we only show half the values,\n",
    "    # instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    # Specifies that we should use the existing axes\n",
    "    ax=ax,\n",
    "    # Specifies that we want labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \"orientation\": \"horizontal\", \"pad\": .2, \"extend\": \"both\"}\n",
    ")\n",
    "\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation amung the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedrooms\n",
    "# sqft_living\n",
    "# condition\n",
    "# grade\n",
    "# lat\n",
    "# sqft_living15\n",
    "# view\n",
    "# sqft_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.price.min())\n",
    "print(data.price.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "\n",
    "ax.hist(y_reg, bins=100)\n",
    "\n",
    "ax.set_xlabel(\"Price\")\n",
    "ax.set_ylabel(\"Amount of Homes at 'Price'\")\n",
    "ax.set_title(\"Distribution of King County House Prices\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "\n",
    "ax.hist(y_train, bins=100)\n",
    "\n",
    "ax.set_xlabel(\"Log Price\")\n",
    "ax.set_ylabel(\"Amount of Homes at 'Log Price'\")\n",
    "ax.set_title(\"Distribution of King County House at Log Prices\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['sqft_living'], data['price'])\n",
    "plt.title('King County, WA. Real Estate Normalized Price')\n",
    "plt.xlabel('Square Feet of Living Space Normal Price')\n",
    "plt.ylabel('Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['sqft_living'], df['log_price']) # changed log_price to price Sunday night 8:40\n",
    "plt.title('King County, WA. Real Estate Logarithmic Price')\n",
    "plt.xlabel('Square Feet of Living Space Log Price')\n",
    "plt.ylabel('Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_correlated_feature = 'sqft_living'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model number one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    "\n",
    "baseline_scores = cross_validate(\n",
    "    estimator=baseline_model,\n",
    "    X=X_train[[most_correlated_feature]],\n",
    "    y=y_train,\n",
    "    return_train_score=True,\n",
    "    cv=splitter\n",
    ")\n",
    "\n",
    "print(\"Train score:     \", baseline_scores[\"train_score\"].mean())\n",
    "print(\"Validation score:\", baseline_scores[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train score:      0.4952998338809739\n",
    "Validation score: 0.4889848978694101\n",
    "\n",
    "Train score:      0.49330734728094866\n",
    "Validation score: 0.4865280709172235\n",
    "\n",
    "-----(above)grade as most_correlate_feature------\n",
    "Train score:      0.4944067972378221\n",
    "Validation score: 0.4875717893880786\n",
    "\n",
    "Train score:      0.49364975800322103\n",
    "Validation score: 0.4820156566590304\n",
    "\n",
    "Train score:      0.4914428975785743\n",
    "Validation score: 0.5074551330857049\n",
    "\n",
    "Train score:      0.48891917325069206\n",
    "Validation score: 0.48598954129972305\n",
    "\n",
    "Train score:      0.4940880327615438\n",
    "Validation score: 0.49475506140480235\n",
    "\n",
    "Train score:      0.4873310788666743\n",
    "Validation score: 0.49769395546845496\n",
    "\n",
    "Train score:      0.49652359839680243\n",
    "Validation score: 0.492600178614152\n",
    "\n",
    "Train score:      0.49162643803585154\n",
    "Validation score: 0.49719265747886765\n",
    "\n",
    "-------Sunday July 11th (above)-----\n",
    "\n",
    "Train score:      0.4892985249004287\n",
    "Validation score: 0.4931173274622296\n",
    "\n",
    "Train score:      0.4963879315073201\n",
    "Validation score: 0.4984831857807322\n",
    "\n",
    "Train score:      0.4847292355963533\n",
    "Validation score: 0.49965463955396056\n",
    "\n",
    "Train score:      0.48451234315928\n",
    "Validation score: 0.49457817817276856\n",
    "\n",
    "Train score:      0.49278350276300226\n",
    "Validation score: 0.48500365490951713\n",
    "\n",
    "Train score:      0.4894444226978396\n",
    "Validation score: 0.49493831279771444\n",
    "\n",
    "Train score:      0.4910537092518262\n",
    "Validation score: 0.4930298774731154\n",
    "\n",
    "Train score:      0.49132625826912263\n",
    "Validation score: 0.4861961020656274\n",
    "\n",
    "Train score:      0.49588251770393416\n",
    "Validation score: 0.4955416312708622\n",
    "\n",
    "Train score:      0.4888862395615335\n",
    "Validation score: 0.49914413903889204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Let's have a look at only numeric columns shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = X_train.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "X_train_numeric.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=8, figsize=(18, 30))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "for index, col in enumerate(X_train_numeric.columns):\n",
    "    ax = axes[index//3][index%3]\n",
    "    ax.scatter(X_train_numeric[col], y_train, alpha=0.2)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"listing price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_second_model = X_train_numeric.drop(columns = [ 'id','sqft_lot', 'floors'\n",
    "                                                       ,'waterfront', 'yr_built','sqft_basement', 'date'\n",
    "                                                       ,'zipcode', 'yr_renovated', 'bathrooms', 'cen_x', 'cen_y'])\n",
    "X_train_second_model.head()\n",
    "\n",
    "# ****view has something in it that the model doesn't like****\n",
    "\n",
    "# bedrooms\n",
    "# sqft_living\n",
    "# grade\n",
    "# lat\n",
    "# sqft_living15\n",
    "#'sqft_above'\n",
    "# condition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = LinearRegression()\n",
    "\n",
    "second_model_scores = cross_validate(\n",
    "    estimator=second_model,\n",
    "    X=X_train_second_model,\n",
    "    y=y_train,\n",
    "    return_train_score=True,\n",
    "    cv=splitter\n",
    ")\n",
    "\n",
    "print(\"Current Model\")\n",
    "print(\"Train score:     \", second_model_scores[\"train_score\"].mean())\n",
    "print(\"Validation score:\", second_model_scores[\"test_score\"].mean())\n",
    "print()\n",
    "print(\"Baseline Model\")\n",
    "print(\"Train score:     \", baseline_scores[\"train_score\"].mean())\n",
    "print(\"Validation score:\", baseline_scores[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that the Train Scores and Validation Scores are relatively near to eachother, it may be said that there is very little multicollinearity in the second_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests from Sunday morning (after data loss debacle and recovery), July 11th"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Current Model\n",
    "Train score:      0.8709627868614471\n",
    "Validation score: 0.8686184338455232\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.4819360632267696\n",
    "Validation score: 0.4715244949587838\n",
    "\n",
    "----Added log_price in place of price (above)-----\n",
    "Current Model\n",
    "Train score:      0.6352729564192802\n",
    "Validation score: 0.6350238827876111\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.48718562431785495\n",
    "Validation score: 0.4933402163328162\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6339620446715306\n",
    "Validation score: 0.6310305321616593\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.49627789748925794\n",
    "Validation score: 0.49963305305594713\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6291489629181085\n",
    "Validation score: 0.6370516347190607\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.49301067096430967\n",
    "Validation score: 0.49655462110875587\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6360677005373939\n",
    "Validation score: 0.6276931826178999\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.49425257982363385\n",
    "Validation score: 0.5005361824066539\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6300949005546357\n",
    "Validation score: 0.6364045061236229\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.4941613018853969\n",
    "Validation score: 0.4948330304130916\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6377109971927726\n",
    "Validation score: 0.626407961904571\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.4965562840429995\n",
    "Validation score: 0.49457533337063414\n",
    "\n",
    "----(above) now dropping sqft_above bcs of p-val in ols --------\n",
    "\n",
    "*after dropped bathrooms*\n",
    "Current Model\n",
    "Train score:      0.6298648177265983\n",
    "Validation score: 0.6341874613330492\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.49156556471551355\n",
    "Validation score: 0.49280209976816375\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6319989507284851\n",
    "Validation score: 0.6331973165449779\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.495925916060342\n",
    "Validation score: 0.487060062605808\n",
    "\n",
    "\n",
    "----dropping bathrooms (above) col due to p-val----\n",
    "Current Model\n",
    "Train score:      0.6316846143732537\n",
    "Validation score: 0.6284291391270559\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.4944067972378221\n",
    "Validation score: 0.4875717893880786\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6318680499907233\n",
    "Validation score: 0.6314545986951546\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.49364975800322103\n",
    "Validation score: 0.4820156566590304\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6355660046824282\n",
    "Validation score: 0.6211650141258881\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.49549165266008366\n",
    "Validation score: 0.49260564533329215\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6361765301829047\n",
    "Validation score: 0.6348878799554117\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.49624342255763104\n",
    "Validation score: 0.48659356161286\n",
    "\n",
    "\n",
    "Current Model\n",
    "Train score:      0.6382527695771348\n",
    "Validation score: 0.6310951746396455\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.4905872982531793\n",
    "Validation score: 0.495396960972789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests from Saturday night, July 10th"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Current Model\n",
    "Train score:      0.6317958757896497\n",
    "Validation score: 0.6325208771260364\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.4963879315073201\n",
    "Validation score: 0.4984831857807322\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Lat and Long excluded"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "without lat and long in test\n",
    "Current Model\n",
    "Train score:      0.5642949051690652\n",
    "Validation score: 0.565226339613679\n",
    "\n",
    "Baseline Model\n",
    "Train score:      0.4963879315073201\n",
    "Validation score: 0.4984831857807322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# linreg = LinearRegression()\n",
    "# linreg.fit(X_train_second_model, y_train)\n",
    "\n",
    "# y_hat_train = linreg.predict(X_train)\n",
    "# y_hat_test = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS and the search for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(y_train, sm.add_constant(X_train_second_model)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importances are based on coefficient magnitude, so\n",
    "# we need to scale the data to normalize the coefficients\n",
    "X_train_for_RFECV = StandardScaler().fit_transform(X_train_second_model)\n",
    "\n",
    "model_for_RFECV = LinearRegression()\n",
    "\n",
    "# Instantiate and fit the selector\n",
    "selector = RFECV(model_for_RFECV, cv=splitter)\n",
    "selector.fit(X_train_for_RFECV, y_train)\n",
    "\n",
    "# Print the results\n",
    "print(\"Was the column selected?\")\n",
    "for index, col in enumerate(X_train_second_model.columns):\n",
    "    print(f\"{col}: {selector.support_[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[[\"bedrooms\",\"sqft_living\"]].corr()) # .57\n",
    "# print(df[[\"bedrooms\",\"condition\"]].corr())# .02\n",
    "# print(df[[\"bedrooms\",\"grade\"]].corr())# .35\n",
    "# print(df[[\"bedrooms\",\"lat\"]].corr())# .009\n",
    "# print(df[[\"bedrooms\",\"sqft_living15\"]].corr()) # .39\n",
    "# print(df[[\"bedrooms\",\"view\"]].corr())# .07 \n",
    "# print(df[[\"bedrooms\",\"sqft_above\"]].corr())# .47\n",
    "\n",
    "# print(df[[\"sqft_living\",\"condition\"]].corr()) # .05\n",
    "# print(df[[\"sqft_living\",\"grade\"]].corr()) # .76 ***********high\n",
    "# print(df[[\"sqft_living\",\"lat\"]].corr()) # .05\n",
    "# print(df[[\"sqft_living\",\"sqft_living15\"]].corr()) # .75 ********high\n",
    "# print(df[[\"sqft_living\",\"view\"]].corr()) # .28 \n",
    "# print(df[[\"sqft_living\",\"sqft_above\"]].corr()) # .87 ********high\n",
    "\n",
    "# print(df[[\"condition\",\"grade\"]].corr()) # -.14\n",
    "# print(df[[\"condition\",\"lat\"]].corr()) # -.015\n",
    "# print(df[[\"condition\",\"sqft_living15\"]].corr()) # -.09\n",
    "# print(df[[\"condition\",\"view\"]].corr()) # .04 \n",
    "# print(df[[\"condition\",\"sqft_above\"]].corr()) # - .10\n",
    "\n",
    "# print(df[[\"grade\",\"lat\"]].corr()) # .11\n",
    "# print(df[[\"grade\",\"sqft_living15\"]].corr()) # .71 *****high\n",
    "# print(df[[\"grade\",\"view\"]].corr()) # .24 \n",
    "# print(df[[\"grade\",\"sqft_above\"]].corr()) # .75 ********high\n",
    "\n",
    "# print(df[[\"lat\",\"sqft_living15\"]].corr()) # .04\n",
    "# print(df[[\"lat\",\"view\"]].corr()) # .006 \n",
    "# print(df[[\"lat\",\"sqft_above\"]].corr()) # -.001\n",
    "\n",
    "# print(df[[\"sqft_living15\",\"view\"]].corr()) # .27 \n",
    "# print(df[[\"sqft_living15\",\"sqft_above\"]].corr()) # .73********high\n",
    "\n",
    "# sqft_living and waterfront are lower than sqft_living an view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['sqft_living', 'view', 'lat'] # minus condition\n",
    "#bedrooms\n",
    "# sqft_living\n",
    "# grade\n",
    "# lat\n",
    "# sqft_living15\n",
    "#'sqft_above'\n",
    "# condition\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sqft_living, condition = 0.49 R2, 0.49 VIF * come back to here\n",
    "sqft_living, lat = 0.64 R2, 6.0 VIF\n",
    "sqft_living, bedrooms = 0.46, 9.4 VIF\n",
    "\n",
    "sqft_living, view, lat = .67, [.68 with np.radians lat and long adjustmen]\n",
    "•VIF•\n",
    "sqft_living - 6.690415\n",
    "view - 1.186125\n",
    "lat - 6.283304\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sqft_living, condition, view = .51 R2,\n",
    "•VIF•\n",
    "sqft_living - 5.358822\n",
    "condition - 5.048914\n",
    "view - 1.172758\n",
    "\n",
    "\n",
    "\n",
    "sqft_living, condition, lat = .668 R2, \n",
    "•VIF•\n",
    "sqft_living - 6.209590\n",
    "condition - 28.611050\n",
    "lat - 35.221892\n",
    "\n",
    "sqft_living, condition, bedrooms = 0.49 R2,\n",
    "•VIF•\n",
    "sqft_living - 9.117693\n",
    "condition - 9.961082\n",
    "bedrooms - 18.252871\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grade, lat = 0.62 R2, 44 VIF\n",
    "grade, condition = 0.50 R2, 15 VIF\n",
    "grade, bedrooms = 0.51 R2, 15 VIF\n",
    "grade, condition = 0.52 R2, 15 VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train[best_features]\n",
    "X_test_final = X_test[best_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  R - Squared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LinearRegression()\n",
    "\n",
    "# fit the model on X_train_final and y_train\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Score the model on X_test_final and y_train\n",
    "final_model.score(X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, final_model.predict(X_test_final), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "print(pd.Series(final_model.coef_, index=X_train_final.columns, name=\"Coefficients\"))\n",
    "print()\n",
    "print(\"Intercept:\", final_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_model.predict(X_test_final) # commented out this line along with below change \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "perfect_line = np.arange(y_test.min(), y_test.max())\n",
    "ax.plot(perfect_line,linestyle=\"--\", color=\"orange\", label=\"Perfect Fit\") # perfect_line, ***removed this from round bracket first position\n",
    "ax.scatter(y_test, preds, alpha=0.5)\n",
    "ax.set_xlabel(\"Actual Price\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QQ plot (Quantile-Quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y_test - preds)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor (Above 5 is bad for your health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = [variance_inflation_factor(X_train_final.values, i) for i in range(X_train_final.shape[1])]\n",
    "pd.Series(vif, index=X_train_final.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Homeoscedasticity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(preds, residuals, alpha=0.5)\n",
    "ax.plot(preds, [0 for i in range(len(X_test))])\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
